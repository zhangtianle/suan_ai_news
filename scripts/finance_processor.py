#!/usr/bin/env python3
"""
Finance News Processor v1
è´¢ç»æ–°é—»æ•°æ®å¤„ç† - æŠ•èµ„è€…è§†è§’

æ ¸å¿ƒåŠŸèƒ½:
1. æ™ºèƒ½åˆ†ç±» - æŒ‰æŠ•èµ„é¢†åŸŸåˆ’åˆ†
2. é‡è¦æ€§è¯„åˆ† - æŠ•èµ„ä»·å€¼å¯¼å‘
3. å¸‚åœºæƒ…ç»ªåˆ†æ - å¤šç©ºä¿¡å·è¯†åˆ«
4. å®ä½“æå– - è‚¡ç¥¨ã€æ¿å—ã€å…¬å¸
"""

import json
import re
from datetime import datetime
from pathlib import Path
from collections import defaultdict

PROJECT_ROOT = Path("/home/admin/.openclaw/workspace/tech-news")
RAW_DIR = PROJECT_ROOT / "data" / "finance" / "raw"
PROCESSED_DIR = PROJECT_ROOT / "data" / "finance" / "processed"

# æŠ•èµ„é¢†åŸŸåˆ†ç±»å…³é”®è¯
INVESTMENT_CATEGORIES = {
    "å®è§‚æ”¿ç­–": [
        "å¤®è¡Œ", "ç¾è”å‚¨", "åˆ©ç‡", "é™æ¯", "åŠ æ¯", "è´§å¸æ”¿ç­–", "è´¢æ”¿æ”¿ç­–",
        "GDP", "CPI", "PMI", "é€šèƒ€", "é€šç¼©", "ç»æµæ•°æ®", "ç»Ÿè®¡å±€",
        "å›½å¸¸ä¼š", "æ”¿æ²»å±€", "å‘æ”¹å§”", "å•†åŠ¡éƒ¨", "è´¢æ”¿éƒ¨",
        "Fed", "FOMC", "ECB", "åˆ©ç‡å†³è®®", "ç¼©è¡¨", "QE"
    ],
    "Aè‚¡å¸‚åœº": [
        "Aè‚¡", "ä¸Šè¯", "æ·±è¯", "åˆ›ä¸šæ¿", "ç§‘åˆ›æ¿", "åŒ—äº¤æ‰€",
        "æ²ªæŒ‡", "æ·±æˆæŒ‡", "ä¸¤å¸‚", "æˆäº¤é¢", "åŒ—å‘èµ„é‡‘", "å—å‘èµ„é‡‘",
        "æ¶¨åœ", "è·Œåœ", "é¾™è™æ¦œ", "æœºæ„", "æ¸¸èµ„", "èèµ„", "èåˆ¸"
    ],
    "ç¾è‚¡å¸‚åœº": [
        "ç¾è‚¡", "çº³æŒ‡", "é“æŒ‡", "æ ‡æ™®", "çº³æ–¯è¾¾å…‹", "çº½äº¤æ‰€",
        "é“ç¼æ–¯", "S&P", "NYSE", "NASDAQ", "åå°”è¡—", "ç¾è”å‚¨",
        "ç§‘æŠ€è‚¡", "ä¸­æ¦‚è‚¡", "ADR", "FAANG", "ä¸ƒå·¨å¤´"
    ],
    "æ¸¯è‚¡å¸‚åœº": [
        "æ¸¯è‚¡", "æ’æŒ‡", "æ’ç”ŸæŒ‡æ•°", "æ¸¯äº¤æ‰€", "HKEX",
        "æ¸¯è‚¡é€š", "æ’ç”Ÿç§‘æŠ€", "è…¾è®¯", "é˜¿é‡Œ", "ç¾å›¢", "å°ç±³"
    ],
    "è¡Œä¸šæ¿å—": [
        "åŠå¯¼ä½“", "èŠ¯ç‰‡", "æ–°èƒ½æº", "å…‰ä¼", "é”‚ç”µæ± ", "å‚¨èƒ½", "é£ç”µ",
        "ç™½é…’", "åŒ»è¯", "ç”Ÿç‰©åˆ¶è¯", "åŒ»ç–—å™¨æ¢°", "ä¸­è¯",
        "é“¶è¡Œ", "åˆ¸å•†", "ä¿é™©", "åœ°äº§", "æˆ¿åœ°äº§",
        "æ±½è½¦", "æ–°èƒ½æºæ±½è½¦", "æ™ºèƒ½é©¾é©¶", "æ±½è½¦é›¶éƒ¨ä»¶",
        "æ¶ˆè´¹ç”µå­", "è‹¹æœäº§ä¸šé“¾", "æ¶ˆè´¹", "é£Ÿå“é¥®æ–™",
        "å†›å·¥", "èˆªå¤©", "é€šä¿¡", "5G", "äººå·¥æ™ºèƒ½", "AI",
        "æœ‰è‰²", "ç…¤ç‚­", "çŸ³æ²¹", "åŒ–å·¥", "é’¢é“", "æ°´æ³¥"
    ],
    "å•†å“æœŸè´§": [
        "æœŸè´§", "å•†å“", "åŸæ²¹", "é»„é‡‘", "ç™½é“¶", "é“œ", "é“",
        "èºçº¹é’¢", "é“çŸ¿çŸ³", "ç„¦ç‚­", "åŠ¨åŠ›ç…¤",
        "å†œäº§å“", "å¤§è±†", "ç‰ç±³", "å°éº¦", "æ£‰èŠ±", "ç™½ç³–",
        "OPEC", "å‡äº§", "å¢äº§", "åº“å­˜", "ä¾›éœ€"
    ],
    "å¤–æ±‡å¸‚åœº": [
        "æ±‡ç‡", "ç¾å…ƒ", "äººæ°‘å¸", "æ¬§å…ƒ", "æ—¥å…ƒ", "è‹±é•‘",
        "USD", "CNY", "EUR", "JPY", "GBP",
        "å¤–æ±‡å‚¨å¤‡", "è´¬å€¼", "å‡å€¼", "æ±‡ç‡æ³¢åŠ¨"
    ],
    "åŸºé‡‘ç†è´¢": [
        "åŸºé‡‘", "å…¬å‹Ÿ", "ç§å‹Ÿ", "ETF", "LOF", "QDII",
        "åŸºé‡‘ç»ç†", "å‡€å€¼", "ç”³è´­", "èµå›", "å®šæŠ•",
        "æƒç›ŠåŸºé‡‘", "å€ºåˆ¸åŸºé‡‘", "è´§å¸åŸºé‡‘", "æŒ‡æ•°åŸºé‡‘"
    ],
    "è´¢æŠ¥ä¸šç»©": [
        "è´¢æŠ¥", "å¹´æŠ¥", "å­£æŠ¥", "ä¸šç»©", "è¥æ”¶", "å‡€åˆ©æ¶¦",
        "æ¯›åˆ©ç‡", "å‡€åˆ©ç‡", "ROE", "EPS", "æ¯è‚¡æ”¶ç›Š",
        "ä¸šç»©é¢„å‘Š", "ä¸šç»©å¿«æŠ¥", "åˆ†æå¸ˆ", "è¯„çº§", "ç ”æŠ¥"
    ],
    "å¹¶è´­é‡ç»„": [
        "å¹¶è´­", "é‡ç»„", "æ”¶è´­", "å€Ÿå£³", "å®šå¢", "é…è‚¡",
        "IPO", "ä¸Šå¸‚", "é€€å¸‚", "ç§æœ‰åŒ–", "åˆ†æ‹†",
        "è‚¡æƒè½¬è®©", "è¦çº¦æ”¶è´­", "åˆå¹¶"
    ],
    "é£é™©é¢„è­¦": [
        "æš´é›·", "è¿çº¦", "é€€å¸‚", "é£é™©", "è°ƒæŸ¥", "å¤„ç½š",
        "è¯‰è®¼", "ä»²è£", "äºæŸ", "å‡å€¼", "åè´¦",
        "è´¨æŠ¼", "å†»ç»“", "ç ´äº§", "æ¸…ç®—"
    ]
}

# é‡è¦å…¬å¸/è‚¡ç¥¨å…³é”®è¯
KEY_ENTITIES = {
    "ç§‘æŠ€å·¨å¤´": ["è‹¹æœ", "å¾®è½¯", "è°·æ­Œ", "Meta", "äºšé©¬é€Š", "ç‰¹æ–¯æ‹‰", "è‹±ä¼Ÿè¾¾", "Netflix"],
    "ä¸­å›½ç§‘æŠ€": ["è…¾è®¯", "é˜¿é‡Œ", "å­—èŠ‚", "ç¾å›¢", "äº¬ä¸œ", "æ‹¼å¤šå¤š", "ç™¾åº¦", "å°ç±³", "å¿«æ‰‹", "Bç«™"],
    "æ–°èƒ½æº": ["å®å¾·æ—¶ä»£", "æ¯”äºšè¿ª", "è”šæ¥", "ç†æƒ³", "å°é¹", "éš†åŸº", "é˜³å…‰ç”µæº"],
    "åŠå¯¼ä½“": ["å°ç§¯ç”µ", "ä¸­èŠ¯å›½é™…", "åè™¹", "åŒ—æ–¹ååˆ›", "éŸ¦å°”è‚¡ä»½"],
    "é‡‘è": ["å·¥å•†é“¶è¡Œ", "å»ºè®¾é“¶è¡Œ", "ä¸­å›½å¹³å®‰", "æ‹›å•†é“¶è¡Œ", "ä¸­ä¿¡è¯åˆ¸", "ä¸œæ–¹è´¢å¯Œ"],
    "æ¶ˆè´¹": ["èŒ…å°", "äº”ç²®æ¶²", "ä¼Šåˆ©", "æµ·å¤©", "ç¾çš„", "æ ¼åŠ›"]
}

# å¸‚åœºå½±å“å…³é”®è¯
MARKET_IMPACT = {
    "é«˜å½±å“": [
        "é™æ¯", "åŠ æ¯", "QE", "ç¼©è¡¨", "åˆ©ç‡å†³è®®",
        "è´¸æ˜“æˆ˜", "åˆ¶è£", "åœ°ç¼˜æ”¿æ²»", "æˆ˜äº‰",
        "ç–«æƒ…", "å°é”", "è¡°é€€", "å±æœº",
        "è´¢æŠ¥è¶…é¢„æœŸ", "ä¸šç»©æš´é›·", "é‡å¤§å¹¶è´­"
    ],
    "ä¸­å½±å“": [
        "æ”¿ç­–", "è§„åˆ’", "è¡¥è´´", "ç›‘ç®¡",
        "ä¸šç»©", "è¥æ”¶", "åˆ©æ¶¦", "è®¢å•",
        "äº§èƒ½", "æ‰©å¼ ", "æŠ•èµ„"
    ],
    "ä½å½±å“": [
        "è§‚ç‚¹", "åˆ†æ", "é¢„æµ‹", "å±•æœ›",
        "æ—¥å¸¸", "å¸¸è§„", "ä¸€èˆ¬"
    ]
}

def categorize_article(article):
    """æŠ•èµ„é¢†åŸŸåˆ†ç±»"""
    title = article.get("title", "")
    url = article.get("url", "")
    source_categories = article.get("categories", [])
    existing_entities = article.get("entities", {})
    
    text = (title + " " + url).lower()
    
    match_scores = {}
    
    # 1. æ¥æºé¢„åˆ†ç±»åŠ åˆ†
    for cat in source_categories:
        cat_mapping = {
            "å®è§‚": "å®è§‚æ”¿ç­–", "æ”¿ç­–": "å®è§‚æ”¿ç­–",
            "è‚¡å¸‚": "Aè‚¡å¸‚åœº", "Aè‚¡": "Aè‚¡å¸‚åœº",
            "ç¾è‚¡": "ç¾è‚¡å¸‚åœº", "æ¸¯è‚¡": "æ¸¯è‚¡å¸‚åœº",
            "æœŸè´§": "å•†å“æœŸè´§", "å•†å“": "å•†å“æœŸè´§",
            "åŸºé‡‘": "åŸºé‡‘ç†è´¢", "å¤–æ±‡": "å¤–æ±‡å¸‚åœº",
            "è´¢æŠ¥": "è´¢æŠ¥ä¸šç»©", "ä¸šç»©": "è´¢æŠ¥ä¸šç»©",
            "èƒ½æº": "å•†å“æœŸè´§", "åŸæ²¹": "å•†å“æœŸè´§"
        }
        mapped = cat_mapping.get(cat, cat)
        if mapped in INVESTMENT_CATEGORIES:
            match_scores[mapped] = match_scores.get(mapped, 0) + 3
    
    # 2. å…³é”®è¯åŒ¹é…
    for category, keywords in INVESTMENT_CATEGORIES.items():
        score = 0
        for keyword in keywords:
            if keyword.lower() in text:
                if keyword.lower() in title.lower():
                    score += 2
                else:
                    score += 1
        
        if score > 0:
            match_scores[category] = match_scores.get(category, 0) + score
    
    # 3. å®ä½“åŒ¹é…
    for entity_type, entities in KEY_ENTITIES.items():
        for entity in entities:
            if entity in title:
                # æ ¹æ®å®ä½“ç±»å‹æ˜ å°„åˆ°åˆ†ç±»
                if entity_type in ["ç§‘æŠ€å·¨å¤´", "ä¸­å›½ç§‘æŠ€"]:
                    match_scores["Aè‚¡å¸‚åœº"] = match_scores.get("Aè‚¡å¸‚åœº", 0) + 1
                elif entity_type == "æ–°èƒ½æº":
                    match_scores["è¡Œä¸šæ¿å—"] = match_scores.get("è¡Œä¸šæ¿å—", 0) + 1
                elif entity_type == "åŠå¯¼ä½“":
                    match_scores["è¡Œä¸šæ¿å—"] = match_scores.get("è¡Œä¸šæ¿å—", 0) + 1
    
    # 4. é£é™©é¢„è­¦ç‰¹æ®Šå¤„ç†
    risk_keywords = MARKET_IMPACT["é«˜å½±å“"] + ["æš´é›·", "è¿çº¦", "é€€å¸‚", "è°ƒæŸ¥", "å¤„ç½š"]
    for kw in risk_keywords:
        if kw in title:
            match_scores["é£é™©é¢„è­¦"] = match_scores.get("é£é™©é¢„è­¦", 0) + 5
    
    # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„åˆ†ç±»
    sorted_cats = sorted(match_scores.items(), key=lambda x: x[1], reverse=True)
    result = [cat for cat, score in sorted_cats[:3] if score >= 2]
    
    return result if result else ["å…¶ä»–"]

def calculate_investment_score(article):
    """è®¡ç®—æŠ•èµ„ä»·å€¼åˆ†æ•°"""
    score = 0
    title = article.get("title", "")
    source = article.get("source", "")
    
    # 1. æ¥æºæƒé‡
    source_weights = {
        # é«˜æƒå¨åª’ä½“
        "è¯åˆ¸æ—¶æŠ¥": 5, "ä¸Šæµ·è¯åˆ¸æŠ¥": 5, "ä¸­å›½è¯åˆ¸æŠ¥": 5, "ç¬¬ä¸€è´¢ç»": 5,
        "è´¢æ–°ç½‘": 5, "21ä¸–çºªç»æµæŠ¥é“": 5, "ç»æµè§‚å¯ŸæŠ¥": 4,
        # å›½é™…æƒå¨
        "Bloomberg": 5, "Reuters": 5, "WSJ": 5, "FT": 5, "CNBC": 4,
        # é—¨æˆ·ç½‘ç«™
        "æ–°æµªè´¢ç»": 4, "ä¸œæ–¹è´¢å¯Œ": 4, "åŒèŠ±é¡º": 3,
        # ä¸“ä¸šåª’ä½“
        "æœŸè´§æ—¥æŠ¥": 4, "ä¸­å›½åŸºé‡‘æŠ¥": 4,
        # ç¤¾åŒº
        "é›ªçƒ": 2, "æ·˜è‚¡å§": 2
    }
    score += source_weights.get(source, 1)
    
    # 2. å¸‚åœºä¿¡å·æƒé‡
    signal = article.get("market_signal", {})
    overall = signal.get("overall", "neutral")
    if overall == "bullish":
        score += 2
    elif overall == "bearish":
        score += 2  # åˆ©ç©ºä¹ŸåŒæ ·é‡è¦
    
    # 3. æ ¸å¿ƒå…³é”®è¯
    core_keywords = [
        # æ”¿ç­–ç±» (æœ€é«˜æƒé‡)
        ("é™æ¯", 5), ("åŠ æ¯", 5), ("åˆ©ç‡å†³è®®", 5), ("è´§å¸æ”¿ç­–", 4),
        ("å›½å¸¸ä¼š", 4), ("æ”¿æ²»å±€ä¼šè®®", 4), ("ç¾è”å‚¨", 5), ("å¤®è¡Œ", 4),
        
        # ä¸šç»©ç±»
        ("è´¢æŠ¥", 4), ("ä¸šç»©", 3), ("è¶…é¢„æœŸ", 4), ("æš´é›·", 5),
        
        # å¸‚åœºç±»
        ("åŒ—å‘èµ„é‡‘", 3), ("æœºæ„", 2), ("é¾™è™æ¦œ", 2),
        
        # å…¬å¸ç±»
        ("èŒ…å°", 3), ("å®å¾·æ—¶ä»£", 3), ("æ¯”äºšè¿ª", 3), ("è…¾è®¯", 3),
        ("è‹±ä¼Ÿè¾¾", 3), ("ç‰¹æ–¯æ‹‰", 3), ("è‹¹æœ", 2),
        
        # é£é™©ç±»
        ("é£é™©", 3), ("è°ƒæŸ¥", 3), ("å¤„ç½š", 3), ("è¿çº¦", 4),
        
        # å›½é™…ç±»
        ("è´¸æ˜“æˆ˜", 4), ("åˆ¶è£", 4), ("åœ°ç¼˜", 3)
    ]
    
    for keyword, weight in core_keywords:
        if keyword in title:
            score += weight
    
    # 4. æ—¶é—´æ•æ„Ÿæ€§
    if any(w in title for w in ["ä»Šæ—¥", "åˆšåˆš", "çªå‘", "é‡ç£…", "ç´§æ€¥"]):
        score += 2
    
    # 5. æ•°æ®å«é‡
    if re.search(r'\d+(\.\d+)?%', title):  # åŒ…å«ç™¾åˆ†æ¯”
        score += 1
    if re.search(r'\d+äº¿', title):  # åŒ…å«é‡‘é¢
        score += 1
    
    # 6. é™æƒ
    if len(title) < 10 or len(title) > 80:
        score -= 1
    
    return max(score, 1)

def extract_key_points(title, content=""):
    """æå–å…³é”®æŠ•èµ„è¦ç‚¹"""
    points = []
    text = title + " " + content
    
    # 1. æ”¿ç­–å½±å“
    if any(w in text for w in ["é™æ¯", "é™å‡†", "å®½æ¾"]):
        points.append("è´§å¸æ”¿ç­–å®½æ¾ä¿¡å·")
    if any(w in text for w in ["åŠ æ¯", "æ”¶ç´§", "ç¼©è¡¨"]):
        points.append("è´§å¸æ”¿ç­–æ”¶ç´§ä¿¡å·")
    
    # 2. è¡Œä¸šæœºä¼š
    sectors = []
    sector_keywords = {
        "åŠå¯¼ä½“": ["åŠå¯¼ä½“", "èŠ¯ç‰‡", "é›†æˆç”µè·¯"],
        "æ–°èƒ½æº": ["æ–°èƒ½æº", "å…‰ä¼", "å‚¨èƒ½", "é”‚ç”µæ± "],
        "åŒ»è¯": ["åŒ»è¯", "åˆ›æ–°è¯", "åŒ»ç–—å™¨æ¢°"],
        "æ¶ˆè´¹": ["æ¶ˆè´¹", "é›¶å”®", "ç™½é…’"],
        "é‡‘è": ["é“¶è¡Œ", "åˆ¸å•†", "ä¿é™©"],
        "åœ°äº§": ["åœ°äº§", "æˆ¿åœ°äº§", "ç‰©ä¸š"]
    }
    
    for sector, keywords in sector_keywords.items():
        if any(kw in text for kw in keywords):
            sectors.append(sector)
    
    if sectors:
        points.append(f"æ¶‰åŠæ¿å—: {', '.join(sectors)}")
    
    # 3. å…¬å¸åŠ¨æ€
    for entity_type, entities in KEY_ENTITIES.items():
        for entity in entities:
            if entity in text:
                points.append(f"å…³æ³¨æ ‡çš„: {entity}")
                break
    
    return points[:3]  # æœ€å¤š3ä¸ªè¦ç‚¹

def process_data():
    """å¤„ç†æ•°æ®"""
    print(f"[{datetime.now().isoformat()}] å¼€å§‹å¤„ç†è´¢ç»æ•°æ®...")
    
    all_articles = []
    today = datetime.now().strftime("%Y-%m-%d")
    
    for file in RAW_DIR.glob("finance_*.json"):
        try:
            with open(file, "r", encoding="utf-8") as f:
                data = json.load(f)
                articles = data.get("articles", [])
                all_articles.extend(articles)
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file}: {e}")
    
    # å»é‡
    seen_ids = set()
    unique_articles = []
    for article in all_articles:
        aid = article.get("id")
        if aid not in seen_ids:
            seen_ids.add(aid)
            unique_articles.append(article)
    
    # åˆ†ç±»å’Œè¯„åˆ†
    categorized = defaultdict(list)
    signal_stats = {"bullish": 0, "bearish": 0, "neutral": 0}
    entity_stats = defaultdict(int)
    sector_stats = defaultdict(int)
    
    for article in unique_articles:
        # æŠ•èµ„åˆ†ç±»
        categories = categorize_article(article)
        article["investment_categories"] = categories
        
        # æŠ•èµ„ä»·å€¼è¯„åˆ†
        article["investment_score"] = calculate_investment_score(article)
        
        # æå–å…³é”®è¦ç‚¹
        article["key_points"] = extract_key_points(article.get("title", ""))
        
        # ç»Ÿè®¡
        for cat in categories:
            categorized[cat].append(article)
        
        signal = article.get("market_signal", {}).get("overall", "neutral")
        signal_stats[signal] += 1
        
        entities = article.get("entities", {})
        for company in entities.get("companies", []):
            entity_stats[company] += 1
        for sector in entities.get("sectors", []):
            sector_stats[sector] += 1
    
    # æ’åº
    for cat in categorized:
        categorized[cat].sort(key=lambda x: x.get("investment_score", 0), reverse=True)
    
    # ä¿å­˜
    PROCESSED_DIR.mkdir(parents=True, exist_ok=True)
    output_file = PROCESSED_DIR / f"processed_{today}.json"
    
    output_data = {
        "date": today,
        "process_time": datetime.now().isoformat(),
        "total_articles": len(unique_articles),
        "categories": {k: len(v) for k, v in categorized.items()},
        "signal_stats": signal_stats,
        "entity_stats": dict(sorted(entity_stats.items(), key=lambda x: x[1], reverse=True)[:20]),
        "sector_stats": dict(sorted(sector_stats.items(), key=lambda x: x[1], reverse=True)[:15]),
        "categorized_articles": dict(categorized),
        "top_articles": sorted(unique_articles, key=lambda x: x.get("investment_score", 0), reverse=True)[:30],
        "risk_articles": categorized.get("é£é™©é¢„è­¦", [])[:10]
    }
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å¤„ç†å®Œæˆ! å…± {len(unique_articles)} æ¡èµ„è®¯")
    print(f"\nğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
    for cat, count in sorted(output_data["categories"].items(), key=lambda x: x[1], reverse=True):
        print(f"   {cat}: {count} æ¡")
    
    print(f"\nğŸ“ˆ å¸‚åœºæƒ…ç»ª:")
    print(f"   åˆ©å¥½: {signal_stats['bullish']} æ¡")
    print(f"   åˆ©ç©º: {signal_stats['bearish']} æ¡")
    print(f"   ä¸­æ€§: {signal_stats['neutral']} æ¡")
    
    print(f"\nğŸ“ ä¿å­˜è‡³: {output_file}")
    
    return output_data

if __name__ == "__main__":
    process_data()