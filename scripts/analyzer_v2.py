#!/usr/bin/env python3
"""
æ¯æ—¥åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨ v2
- å¢åŠ æ¥æºé“¾æ¥
- å¢åŠ å‘å¸ƒæ—¶é—´
- ä¼˜åŒ–æŠ¥å‘Šæ ¼å¼
"""

import json
import os
from datetime import datetime
from pathlib import Path
from collections import defaultdict

PROJECT_ROOT = Path("/home/admin/.openclaw/workspace/tech-news")
PROCESSED_DIR = PROJECT_ROOT / "data" / "processed"
OUTPUT_DIR = PROJECT_ROOT / "output"

def load_processed_data():
    """åŠ è½½å¤„ç†åçš„æ•°æ®"""
    today = datetime.now().strftime("%Y-%m-%d")
    file_path = PROCESSED_DIR / f"processed_{today}.json"
    
    if not file_path.exists():
        files = sorted(PROCESSED_DIR.glob("processed_*.json"), reverse=True)
        if files:
            file_path = files[0]
    
    if file_path.exists():
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    
    return None

def format_pub_date(pub_date):
    """æ ¼å¼åŒ–å‘å¸ƒæ—¶é—´"""
    if not pub_date:
        return "æœªçŸ¥æ—¶é—´"
    
    # å¦‚æœæ˜¯å®Œæ•´æ—¶é—´æ ¼å¼
    if len(pub_date) > 10:
        try:
            dt = datetime.strptime(pub_date[:19], "%Y-%m-%d %H:%M:%S")
            now = datetime.now()
            diff = (now - dt).total_seconds()
            
            if diff < 3600:  # 1å°æ—¶å†…
                return f"{int(diff/60)}åˆ†é’Ÿå‰"
            elif diff < 86400:  # 24å°æ—¶å†…
                return f"{int(diff/3600)}å°æ—¶å‰"
            else:
                return dt.strftime("%m-%d %H:%M")
        except:
            pass
    
    return pub_date

def generate_summary(data):
    """ç”Ÿæˆ500å­—æ‘˜è¦ - å¸¦æ¥æºé“¾æ¥å’Œå‘å¸ƒæ—¶é—´"""
    date = data.get("date", datetime.now().strftime("%Y-%m-%d"))
    total = data.get("total_articles", 0)
    categories = data.get("categories", {})
    top_articles = data.get("top_articles", [])[:10]
    
    summary = f"""# ç§‘æŠ€èµ„è®¯æ—¥æŠ¥ - {date}

## ğŸ“Š ä»Šæ—¥æ¦‚è§ˆ
- **æ€»èµ„è®¯æ•°**: {total} æ¡
- **ä¸»è¦åˆ†ç±»**: {', '.join([f"{k}({v}æ¡)" for k, v in list(categories.items())[:5]])}

## ğŸ”¥ çƒ­ç‚¹èšç„¦
"""
    
    for i, article in enumerate(top_articles[:5], 1):
        title = article.get("title", "")[:60]
        source = article.get("source", "")
        url = article.get("url", "")
        pub_date = article.get("pub_date", "")
        pub_time = format_pub_date(pub_date)
        
        summary += f"{i}. **[{title}]({url})**\n"
        summary += f"   - ğŸ“° {source} | â° {pub_time}\n\n"
    
    summary += "## ğŸ’¡ ä»Šæ—¥è¦ç‚¹\n"
    
    categorized = data.get("categorized_articles", {})
    
    # AI ç›¸å…³
    ai_articles = categorized.get("AI", [])[:5]
    if ai_articles:
        summary += "\n### ğŸ¤– AI/å¤§æ¨¡å‹\n\n"
        for a in ai_articles:
            title = a.get("title", "")[:55]
            url = a.get("url", "")
            source = a.get("source", "")
            pub_date = a.get("pub_date", "")
            pub_time = format_pub_date(pub_date)
            summary += f"- [{title}]({url})\n  *{source} Â· {pub_time}*\n\n"
    
    # èŠ¯ç‰‡ç›¸å…³
    chip_articles = categorized.get("èŠ¯ç‰‡", [])[:5]
    if chip_articles:
        summary += "\n### ğŸ’» èŠ¯ç‰‡/ç®—åŠ›\n\n"
        for a in chip_articles:
            title = a.get("title", "")[:55]
            url = a.get("url", "")
            source = a.get("source", "")
            pub_date = a.get("pub_date", "")
            pub_time = format_pub_date(pub_date)
            summary += f"- [{title}]({url})\n  *{source} Â· {pub_time}*\n\n"
    
    # äº’è”ç½‘ç›¸å…³
    internet_articles = categorized.get("äº’è”ç½‘", [])[:5]
    if internet_articles:
        summary += "\n### ğŸŒ äº’è”ç½‘\n\n"
        for a in internet_articles:
            title = a.get("title", "")[:55]
            url = a.get("url", "")
            source = a.get("source", "")
            pub_date = a.get("pub_date", "")
            pub_time = format_pub_date(pub_date)
            summary += f"- [{title}]({url})\n  *{source} Â· {pub_time}*\n\n"
    
    summary += f"""

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*
"""
    
    return summary

def generate_detailed_report(data):
    """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š - å¸¦æ¥æºé“¾æ¥å’Œå‘å¸ƒæ—¶é—´"""
    date = data.get("date", datetime.now().strftime("%Y-%m-%d"))
    total = data.get("total_articles", 0)
    categories = data.get("categories", {})
    top_articles = data.get("top_articles", [])
    categorized = data.get("categorized_articles", {})
    
    report = f"""# ç§‘æŠ€èµ„è®¯æ·±åº¦åˆ†ææŠ¥å‘Š
## {date}

---

## ä¸€ã€ä»Šæ—¥æ•°æ®æ¦‚è§ˆ

æœ¬æ—¥å…±æ”¶é›†ç§‘æŠ€èµ„è®¯ **{total}** æ¡ï¼Œæ¥æºæ¶µç›–å›½å†…å¤–ä¸»æµç§‘æŠ€åª’ä½“ã€‚

### åˆ†ç±»åˆ†å¸ƒ
"""
    
    for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total * 100) if total > 0 else 0
        report += f"- **{cat}**: {count} æ¡ ({percentage:.1f}%)\n"
    
    report += """

---

## äºŒã€é‡ç‚¹æ–°é—»æ·±åº¦è§£è¯»

"""
    
    for i, article in enumerate(top_articles[:12], 1):
        title = article.get("title", "æœªçŸ¥æ ‡é¢˜")
        source = article.get("source", "æœªçŸ¥æ¥æº")
        url = article.get("url", "")
        score = article.get("importance_score", 0)
        cats = article.get("auto_categories", [])
        pub_date = article.get("pub_date", "")
        pub_time = format_pub_date(pub_date)
        
        report += f"""### {i}. {title}

"""
        report += f"| å±æ€§ | å†…å®¹ |\n"
        report += f"|------|------|\n"
        report += f"| ğŸ“° **æ¥æº** | {source} |\n"
        report += f"| â° **æ—¶é—´** | {pub_time} |\n"
        report += f"| ğŸ·ï¸ **åˆ†ç±»** | {', '.join(cats)} |\n"
        report += f"| â­ **é‡è¦æ€§** | {'â­' * min(score, 5)} |\n"
        report += f"| ğŸ”— **é“¾æ¥** | [ç‚¹å‡»æŸ¥çœ‹åŸæ–‡]({url}) |\n\n"
        
        # æ·»åŠ ç®€è¦åˆ†æ
        if "å‘å¸ƒ" in title or "æ¨å‡º" in title:
            report += "> ğŸ“¢ **äº§å“å‘å¸ƒåŠ¨æ€** - å€¼å¾—å…³æ³¨çš„æ–°äº§å“/æ–°åŠŸèƒ½å‘å¸ƒ\n\n"
        elif "èèµ„" in title or "æŠ•èµ„" in title:
            report += "> ğŸ’° **èµ„æœ¬åŠ¨æ€** - è¡Œä¸šèµ„æœ¬æµå‘å€¼å¾—å…³æ³¨\n\n"
        elif "çªç ´" in title or "é¦–æ¬¡" in title:
            report += "> ğŸš€ **æŠ€æœ¯çªç ´** - è¡Œä¸šé‡Œç¨‹ç¢‘äº‹ä»¶\n\n"
        elif "å¼€æº" in title:
            report += "> ğŸ”“ **å¼€æºåŠ¨æ€** - å¼€æºç¤¾åŒºé‡è¦è¿›å±•\n\n"
        
        report += "---\n\n"
    
    # åˆ†ç±»æ·±åº¦åˆ†æ
    report += """## ä¸‰ã€è¡Œä¸šè¶‹åŠ¿åˆ†æ

"""
    
    # AIæ¿å—
    ai_articles = categorized.get("AI", [])
    if ai_articles:
        report += f"""### ğŸ¤– AI/å¤§æ¨¡å‹æ¿å— ({len(ai_articles)}æ¡)

æœ¬æ—¥AIç›¸å…³èµ„è®¯å…±{len(ai_articles)}æ¡ï¼Œä¸»è¦æ¶‰åŠï¼š

"""
        for a in ai_articles[:10]:
            title = a.get("title", "")[:50]
            url = a.get("url", "")
            source = a.get("source", "")
            pub_time = format_pub_date(a.get("pub_date", ""))
            report += f"- [{title}]({url}) *{source} Â· {pub_time}*\n"
        
        report += "\n**è¶‹åŠ¿æ´å¯Ÿ**: "
        
        ai_titles = " ".join([a.get("title", "") for a in ai_articles])
        insights = []
        if "å¼€æº" in ai_titles:
            insights.append("å¼€æºæ¨¡å‹æŒç»­æ´»è·ƒï¼Œç¤¾åŒºç”Ÿæ€ç¹è£å‘å±•")
        if "Agent" in ai_titles or "æ™ºèƒ½ä½“" in ai_titles:
            insights.append("AI Agentæˆä¸ºæ–°çš„ç«äº‰ç„¦ç‚¹ï¼Œå„å¤§å‚å•†åŠ é€Ÿå¸ƒå±€")
        if "å¤šæ¨¡æ€" in ai_titles:
            insights.append("å¤šæ¨¡æ€æŠ€æœ¯å¿«é€Ÿæ¼”è¿›ï¼Œåº”ç”¨åœºæ™¯ä¸æ–­æ‹“å±•")
        if "æ¨ç†" in ai_titles:
            insights.append("æ¨ç†èƒ½åŠ›æˆä¸ºæ¨¡å‹ç«äº‰æ–°æˆ˜åœº")
        
        if insights:
            report += "ï¼›".join(insights) + "ã€‚"
        else:
            report += "AIé¢†åŸŸæŒç»­å¿«é€Ÿå‘å±•ï¼Œå»ºè®®å…³æ³¨å¤´éƒ¨ç©å®¶åŠ¨æ€ã€‚"
        
        report += "\n\n"
    
    # èŠ¯ç‰‡æ¿å—
    chip_articles = categorized.get("èŠ¯ç‰‡", [])
    if chip_articles:
        report += f"""### ğŸ’» èŠ¯ç‰‡/ç®—åŠ›æ¿å— ({len(chip_articles)}æ¡)

"""
        for a in chip_articles[:8]:
            title = a.get("title", "")[:50]
            url = a.get("url", "")
            source = a.get("source", "")
            pub_time = format_pub_date(a.get("pub_date", ""))
            report += f"- [{title}]({url}) *{source} Â· {pub_time}*\n"
        report += "\n"
    
    # äº’è”ç½‘æ¿å—
    internet_articles = categorized.get("äº’è”ç½‘", [])
    if internet_articles:
        report += f"""### ğŸŒ äº’è”ç½‘/å·¨å¤´æ¿å— ({len(internet_articles)}æ¡)

"""
        for a in internet_articles[:8]:
            title = a.get("title", "")[:50]
            url = a.get("url", "")
            source = a.get("source", "")
            pub_time = format_pub_date(a.get("pub_date", ""))
            report += f"- [{title}]({url}) *{source} Â· {pub_time}*\n"
        report += "\n"
    
    # æŠ•èµ„æ¿å—
    invest_articles = categorized.get("åˆ›ä¸šæŠ•èµ„", [])
    if invest_articles:
        report += f"""### ğŸ’µ æŠ•èµ„/èèµ„æ¿å— ({len(invest_articles)}æ¡)

"""
        for a in invest_articles[:8]:
            title = a.get("title", "")[:50]
            url = a.get("url", "")
            source = a.get("source", "")
            pub_time = format_pub_date(a.get("pub_date", ""))
            report += f"- [{title}]({url}) *{source} Â· {pub_time}*\n"
        report += "\n"
    
    # å¼€æºæ¿å—
    open_source_articles = categorized.get("å¼€æº", [])
    if open_source_articles:
        report += f"""### ğŸ”“ å¼€æºæ¿å— ({len(open_source_articles)}æ¡)

"""
        for a in open_source_articles[:5]:
            title = a.get("title", "")[:50]
            url = a.get("url", "")
            source = a.get("source", "")
            pub_time = format_pub_date(a.get("pub_date", ""))
            report += f"- [{title}]({url}) *{source} Â· {pub_time}*\n"
        report += "\n"
    
    report += f"""---

## å››ã€æ˜æ—¥å…³æ³¨ç‚¹

åŸºäºä»Šæ—¥æ•°æ®åˆ†æï¼Œå»ºè®®å…³æ³¨ä»¥ä¸‹æ–¹å‘ï¼š

1. **å¤§æ¨¡å‹ç«äº‰æ ¼å±€** - å…³æ³¨OpenAIã€Googleã€Anthropicç­‰å¤´éƒ¨ç©å®¶åŠ¨æ€
2. **å›½äº§ç®—åŠ›çªç ´** - èŠ¯ç‰‡è‡ªä¸»å¯æ§è¿›ç¨‹å€¼å¾—æŒç»­è·Ÿè¸ª
3. **åº”ç”¨è½åœ°è¿›å±•** - AIåº”ç”¨å•†ä¸šåŒ–è¿›å…¥å…³é”®æœŸ
4. **èµ„æœ¬æµå‘å˜åŒ–** - æŠ•èµ„çƒ­ç‚¹å¯èƒ½é¢„ç¤ºä¸‹ä¸€æ³¢é£å£

---

## äº”ã€æ•°æ®æ¥æº

æœ¬æŠ¥å‘Šæ•°æ®æ¥æºäºå›½å†…å¤–ä¸»æµç§‘æŠ€åª’ä½“ï¼š

"""
    
    sources = set(a.get("source", "") for a in top_articles)
    for source in sorted(sources):
        report += f"- {source}\n"
    
    report += f"""

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*  
*ç”± Tech News Aggregator v2 è‡ªåŠ¨ç”Ÿæˆ*
"""
    
    return report

def main():
    """ä¸»å‡½æ•°"""
    print(f"[{datetime.now().isoformat()}] å¼€å§‹ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    
    data = load_processed_data()
    
    if not data:
        print("é”™è¯¯: æœªæ‰¾åˆ°å¤„ç†åçš„æ•°æ®")
        return
    
    summary = generate_summary(data)
    detailed = generate_detailed_report(data)
    
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    date = data.get("date", datetime.now().strftime("%Y-%m-%d"))
    
    year_month = date[:7]
    day_dir = OUTPUT_DIR / year_month
    day_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜æ‘˜è¦
    summary_file = day_dir / f"summary_{date}.md"
    with open(summary_file, "w", encoding="utf-8") as f:
        f.write(summary)
    print(f"âœ… æ‘˜è¦å·²ä¿å­˜: {summary_file}")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    detailed_file = day_dir / f"detailed_{date}.md"
    with open(detailed_file, "w", encoding="utf-8") as f:
        f.write(detailed)
    print(f"âœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {detailed_file}")
    
    # ä¿å­˜JSON
    json_file = day_dir / f"report_{date}.json"
    report_data = {
        "date": date,
        "generated_at": datetime.now().isoformat(),
        "summary": summary,
        "detailed_report": detailed,
        "stats": {
            "total_articles": data.get("total_articles", 0),
            "categories": data.get("categories", {})
        }
    }
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ‰ æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
    
    return report_data

if __name__ == "__main__":
    main()