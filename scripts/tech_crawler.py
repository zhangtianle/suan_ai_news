#!/usr/bin/env python3
"""
Tech News Crawler v6 - å›½é™…ç‰ˆ
- æ–°å¢æµ·å¤–ç»¼åˆåŠèµ„è®¯ç±»ç½‘ç«™
- å¢åŠ å‘å¸ƒæ—¶é—´æå–
- æ”¯æŒ RSS è®¢é˜…æº
"""

import json
import re
import time
import random
from datetime import datetime, timezone, timedelta
from pathlib import Path
import hashlib
import subprocess
import html
import xml.etree.ElementTree as ET

PROJECT_ROOT = Path("/home/admin/.openclaw/workspace/tech-news")
DATA_DIR = PROJECT_ROOT / "data" / "raw"
LOGS_DIR = PROJECT_ROOT / "logs"

# å›½å†…ç§‘æŠ€åª’ä½“
CN_SOURCES = [
    {"name": "ITä¹‹å®¶", "url": "https://www.ithome.com", "type": "tech_news", "priority": "high", "category": ["ç§‘æŠ€", "æ•°ç "]},
    {"name": "InfoQ", "url": "https://www.infoq.cn", "type": "tech_media", "priority": "high", "category": ["æŠ€æœ¯", "æ¶æ„"]},
    {"name": "é‡å­ä½", "url": "https://www.qbitai.com", "type": "ai_media", "priority": "high", "category": ["AI", "ç§‘æŠ€"]},
    {"name": "è™å—…", "url": "https://www.huxiu.com", "type": "media", "priority": "medium", "category": ["ç§‘æŠ€", "å•†ä¸š"]},
    {"name": "æ™ºæºç¤¾åŒº", "url": "https://hub.baai.ac.cn", "type": "ai_research", "priority": "high", "category": ["AI", "ç ”ç©¶"]},
    {"name": "é›·é”‹ç½‘", "url": "https://www.leiphone.com", "type": "ai_media", "priority": "high", "category": ["AI", "ç§‘æŠ€"]},
    {"name": "PingWest", "url": "https://www.pingwest.com", "type": "tech_media", "priority": "medium", "category": ["ç§‘æŠ€", "äº’è”ç½‘"]},
    {"name": "çˆ±èŒƒå„¿", "url": "https://www.ifanr.com", "type": "tech_media", "priority": "medium", "category": ["ç§‘æŠ€", "æ•°ç "]},
    {"name": "é©±åŠ¨ä¹‹å®¶", "url": "https://www.mydrivers.com", "type": "tech_news", "priority": "medium", "category": ["ç§‘æŠ€", "ç¡¬ä»¶"]},
    {"name": "36æ°ª", "url": "https://36kr.com", "type": "startup_media", "priority": "high", "category": ["åˆ›ä¸š", "æŠ•èµ„"]},
    {"name": "æœºå™¨ä¹‹å¿ƒ", "url": "https://www.jiqizhixin.com", "type": "ai_media", "priority": "high", "category": ["AI", "ç ”ç©¶"]},
]

# æµ·å¤–ç§‘æŠ€åª’ä½“
INTL_SOURCES = [
    # ä¸»æµç§‘æŠ€åª’ä½“
    {"name": "TechCrunch", "url": "https://techcrunch.com", "type": "international", "priority": "high", "category": ["ç§‘æŠ€", "åˆ›ä¸š", "å›½é™…"], "rss": "https://techcrunch.com/feed/"},
    {"name": "TheVerge", "url": "https://www.theverge.com", "type": "international", "priority": "high", "category": ["ç§‘æŠ€", "æ•°ç ", "å›½é™…"], "rss": "https://www.theverge.com/rss/index.xml"},
    {"name": "Wired", "url": "https://www.wired.com", "type": "international", "priority": "high", "category": ["ç§‘æŠ€", "æ–‡åŒ–", "å›½é™…"], "rss": "https://www.wired.com/feed/rss"},
    {"name": "Ars Technica", "url": "https://arstechnica.com", "type": "international", "priority": "high", "category": ["ç§‘æŠ€", "æŠ€æœ¯", "å›½é™…"], "rss": "https://feeds.arstechnica.com/arstechnica/index"},
    {"name": "Engadget", "url": "https://www.engadget.com", "type": "international", "priority": "high", "category": ["ç§‘æŠ€", "æ•°ç ", "å›½é™…"], "rss": "https://www.engadget.com/rss.xml"},
    {"name": "VentureBeat", "url": "https://venturebeat.com", "type": "international", "priority": "high", "category": ["ç§‘æŠ€", "AI", "å›½é™…"], "rss": "https://venturebeat.com/feed/"},
    {"name": "MIT Technology Review", "url": "https://www.technologyreview.com", "type": "international", "priority": "high", "category": ["ç§‘æŠ€", "ç ”ç©¶", "å›½é™…"], "rss": "https://www.technologyreview.com/feed/"},
    
    # AI ä¸“ä¸šåª’ä½“
    {"name": "TheRegister", "url": "https://www.theregister.com", "type": "international", "priority": "medium", "category": ["ç§‘æŠ€", "IT", "å›½é™…"], "rss": "https://www.theregister.com/headlines.atom"},
    
    # ç»¼åˆæ–°é—»ç§‘æŠ€ç‰ˆ
    {"name": "BBC Technology", "url": "https://www.bbc.com/news/technology", "type": "international", "priority": "high", "category": ["ç§‘æŠ€", "æ–°é—»", "å›½é™…"], "rss": "https://feeds.bbci.co.uk/news/technology/rss.xml"},
    {"name": "Reuters Tech", "url": "https://www.reuters.com/technology/", "type": "international", "priority": "high", "category": ["ç§‘æŠ€", "æ–°é—»", "å›½é™…"], "rss": "https://www.reutersagency.com/feed/?taxonomy=best-topics&post_type=best"},
    
    # å¼€å‘è€…ç¤¾åŒº
    {"name": "Hacker News", "url": "https://news.ycombinator.com", "type": "community", "priority": "high", "category": ["æŠ€æœ¯", "å¼€æº", "å›½é™…"], "rss": "https://hnrss.org/frontpage"},
    {"name": "Dev.to", "url": "https://dev.to", "type": "community", "priority": "medium", "category": ["æŠ€æœ¯", "å¼€å‘", "å›½é™…"], "rss": "https://dev.to/feed"},
    
    # AI/ML ä¸“ä¸š
    {"name": "AI News", "url": "https://www.artificialintelligence-news.com", "type": "ai_media", "priority": "medium", "category": ["AI", "å›½é™…"], "rss": "https://www.artificialintelligence-news.com/feed/"},
    {"name": "Synced", "url": "https://syncedreview.com", "type": "ai_media", "priority": "high", "category": ["AI", "ç ”ç©¶", "å›½é™…"], "rss": "https://syncedreview.com/feed/"},
]

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 Chrome/120.0.0.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
]

def log(msg, level="INFO"):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] [{level}] {msg}")
    try:
        with open(LOGS_DIR / f"crawler_{datetime.now().strftime('%Y%m%d')}.log", "a", encoding="utf-8") as f:
            f.write(f"[{timestamp}] [{level}] {msg}\n")
    except:
        pass

def generate_id(text):
    return hashlib.md5(text.encode()).hexdigest()[:12]

def clean_text(text):
    if not text:
        return ""
    text = re.sub(r'<[^>]+>', '', text)
    try:
        text = html.unescape(text)
    except:
        pass
    return re.sub(r'\s+', ' ', text).strip()

def parse_pub_date(date_str):
    """è§£æå„ç§æ ¼å¼çš„å‘å¸ƒæ—¶é—´"""
    if not date_str:
        return None
    
    # å¸¸è§æ—¥æœŸæ ¼å¼
    formats = [
        "%a, %d %b %Y %H:%M:%S %z",  # RFC 2822
        "%a, %d %b %Y %H:%M:%S GMT",
        "%Y-%m-%dT%H:%M:%S%z",       # ISO 8601
        "%Y-%m-%dT%H:%M:%SZ",
        "%Y-%m-%d %H:%M:%S",
        "%Y-%m-%d",
    ]
    
    for fmt in formats:
        try:
            dt = datetime.strptime(date_str.strip(), fmt)
            # è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´
            if dt.tzinfo:
                dt = dt.astimezone(timezone(timedelta(hours=8)))
            return dt.strftime("%Y-%m-%d %H:%M:%S")
        except:
            continue
    
    # å°è¯•æå–æ—¥æœŸéƒ¨åˆ†
    match = re.search(r'(\d{4}-\d{2}-\d{2})', date_str)
    if match:
        return match.group(1)
    
    return None

def fetch_rss(source):
    """é€šè¿‡ RSS è·å–æ•°æ®"""
    rss_url = source.get("rss")
    if not rss_url:
        return []
    
    log(f"RSS è·å–: {source['name']}")
    
    try:
        cmd = ["curl", "-s", "-L",
               "--connect-timeout", "15",
               "--max-time", "30",
               "-H", f"User-Agent: {random.choice(USER_AGENTS)}",
               rss_url]
        
        result = subprocess.run(cmd, capture_output=True, timeout=45)
        
        if not result.stdout or len(result.stdout) < 100:
            return []
        
        content = result.stdout.decode('utf-8', errors='replace')
        
        # è§£æ XML
        articles = []
        root = ET.fromstring(content)
        
        # å¤„ç† RSS 2.0 æ ¼å¼
        for item in root.findall('.//item')[:50]:
            try:
                title_elem = item.find('title')
                link_elem = item.find('link')
                pub_date_elem = item.find('pubDate')
                desc_elem = item.find('description')
                
                if title_elem is None or link_elem is None:
                    continue
                
                title = clean_text(title_elem.text) if title_elem.text else ""
                url = link_elem.text.strip() if link_elem.text else ""
                pub_date = parse_pub_date(pub_date_elem.text) if pub_date_elem is not None else None
                
                if not title or len(title) < 5 or not url:
                    continue
                
                article = {
                    "id": generate_id(url + title),
                    "title": title,
                    "url": url,
                    "source": source["name"],
                    "categories": source.get("category", []),
                    "pub_date": pub_date,  # å‘å¸ƒæ—¶é—´
                    "crawl_time": datetime.now().isoformat(),
                    "date": datetime.now().strftime("%Y-%m-%d")
                }
                
                articles.append(article)
                
            except Exception as e:
                continue
        
        log(f"{source['name']} (RSS): {len(articles)} æ¡")
        return articles
        
    except subprocess.TimeoutExpired:
        log(f"RSS è¶…æ—¶: {source['name']}", "WARN")
        return []
    except Exception as e:
        log(f"RSS é”™è¯¯ {source['name']}: {str(e)[:50]}", "WARN")
        return []

def fetch_html(source):
    """é€šè¿‡ HTML é¡µé¢è·å–æ•°æ®"""
    url = source["url"]
    name = source["name"]
    
    log(f"HTML è·å–: {name}")
    
    try:
        cmd = ["curl", "-s", "-L",
               "--connect-timeout", "10",
               "--max-time", "25",
               "-H", f"User-Agent: {random.choice(USER_AGENTS)}",
               "-H", "Accept: text/html,*/*",
               url]
        
        result = subprocess.run(cmd, capture_output=True, timeout=35)
        
        if not result.stdout or len(result.stdout) < 500:
            return []
        
        # è§£ç 
        for enc in ['utf-8', 'gbk', 'gb2312', 'latin-1']:
            try:
                content = result.stdout.decode(enc)
                break
            except:
                continue
        else:
            content = result.stdout.decode('utf-8', errors='replace')
        
        # æå–é“¾æ¥
        pattern = r'<a[^>]*href=["\']([^"\']+)["\'][^>]*>([^<]+)</a>'
        links = re.findall(pattern, content, re.IGNORECASE)
        
        base_domain = re.match(r'https?://([^/]+)', url)
        base_domain = base_domain.group(1) if base_domain else ""
        
        articles = []
        seen_urls = set()
        
        for href, title in links[:150]:
            href = href.strip()
            if href.startswith("/"):
                href = f"https://{base_domain}{href}"
            elif not href.startswith("http"):
                continue
            
            if href in seen_urls:
                continue
            seen_urls.add(href)
            
            title = clean_text(title)
            
            # å°è¯•ä» HTML æå–å‘å¸ƒæ—¶é—´
            pub_date = None
            # å¸¸è§æ—¶é—´æ ¼å¼æ¨¡å¼
            time_patterns = [
                r'<time[^>]*datetime=["\']([^"\']+)["\']',
                r'<span[^>]*class="[^"]*date[^"]*"[^>]*>([^<]+)</span>',
                r'data-date="([^"]+)"',
                r'pubdate="([^"]+)"',
            ]
            
            article = {
                "id": generate_id(href + title),
                "title": title,
                "url": href,
                "source": name,
                "categories": source.get("category", []),
                "pub_date": pub_date,  # å‘å¸ƒæ—¶é—´
                "crawl_time": datetime.now().isoformat(),
                "date": datetime.now().strftime("%Y-%m-%d")
            }
            
            # éªŒè¯æ–‡ç« 
            if validate_article(article):
                articles.append(article)
            
            if len(articles) >= 80:
                break
        
        log(f"{name} (HTML): {len(articles)} æ¡")
        return articles
        
    except subprocess.TimeoutExpired:
        log(f"HTML è¶…æ—¶: {name}", "WARN")
        return []
    except Exception as e:
        log(f"HTML é”™è¯¯ {name}: {str(e)[:50]}", "ERROR")
        return []

def validate_article(article):
    """éªŒè¯æ–‡ç« æœ‰æ•ˆæ€§"""
    title = article.get("title", "")
    url = article.get("url", "")
    
    if not title or len(title) < 5 or len(title) > 200:
        return False
    if not url or len(url) < 10:
        return False
    
    skip_words = ["ç™»å½•", "æ³¨å†Œ", "é¦–é¡µ", "æ›´å¤š", "åˆ†äº«", "æ”¶è—", "å¾®ä¿¡", "å¾®åš", "APP", 
                  "ä¸‹ä¸€é¡µ", "ä¸Šä¸€é¡µ", "Subscribe", "Login", "Sign Up", "RSS", "About"]
    if any(w in title for w in skip_words):
        return False
    
    return True

def crawl_source(source):
    """çˆ¬å–å•ä¸ªæ•°æ®æº"""
    # ä¼˜å…ˆä½¿ç”¨ RSS
    if source.get("rss"):
        articles = fetch_rss(source)
        if articles:
            return articles
    
    # RSS å¤±è´¥åˆ™ç”¨ HTML
    return fetch_html(source)

def main():
    log("=" * 60)
    log("ç§‘æŠ€èµ„è®¯çˆ¬è™« v6 å¯åŠ¨ (å›½é™…å¢å¼ºç‰ˆ)")
    log("=" * 60)
    
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    LOGS_DIR.mkdir(parents=True, exist_ok=True)
    
    all_articles = []
    
    # çˆ¬å–å›½å†…æº
    log("\nğŸ“ çˆ¬å–å›½å†…æ•°æ®æº...")
    for source in CN_SOURCES:
        try:
            articles = crawl_source(source)
            all_articles.extend(articles)
            time.sleep(random.uniform(0.3, 0.8))
        except Exception as e:
            log(f"çˆ¬å– {source['name']} å¼‚å¸¸: {e}", "ERROR")
    
    # çˆ¬å–å›½é™…æº
    log("\nğŸŒ çˆ¬å–å›½é™…æ•°æ®æº...")
    for source in INTL_SOURCES:
        try:
            articles = crawl_source(source)
            all_articles.extend(articles)
            time.sleep(random.uniform(0.5, 1.0))
        except Exception as e:
            log(f"çˆ¬å– {source['name']} å¼‚å¸¸: {e}", "ERROR")
    
    # å»é‡
    seen = set()
    unique = []
    for a in all_articles:
        key = a["id"] + a["title"][:20]
        if key not in seen:
            seen.add(key)
            unique.append(a)
    
    # ç»Ÿè®¡
    source_stats = {}
    for a in unique:
        src = a.get("source", "æœªçŸ¥")
        source_stats[src] = source_stats.get(src, 0) + 1
    
    # ä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = DATA_DIR / f"news_{timestamp}.json"
    
    output_data = {
        "crawl_time": datetime.now().isoformat(),
        "version": "v6",
        "total_articles": len(unique),
        "cn_sources": [s["name"] for s in CN_SOURCES],
        "intl_sources": [s["name"] for s in INTL_SOURCES],
        "source_stats": source_stats,
        "articles": unique
    }
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    log("\n" + "=" * 60)
    log(f"âœ… å®Œæˆ! å…± {len(unique)} æ¡èµ„è®¯")
    log(f"ğŸ“ ä¿å­˜: {output_file}")
    log("\nğŸ“Š æ•°æ®æºç»Ÿè®¡:")
    for src, count in sorted(source_stats.items(), key=lambda x: x[1], reverse=True)[:10]:
        log(f"   {src}: {count} æ¡")
    
    return unique

if __name__ == "__main__":
    main()