#!/usr/bin/env python3
"""
Finance News Crawler v1
è´¢ç»æ–°é—»çˆ¬è™« - æŠ•èµ„è€…è§†è§’

æ•°æ®æºåˆ†ç±»:
- å®è§‚ç»æµ: å¤®è¡Œæ”¿ç­–ã€ç»æµæ•°æ®ã€å›½é™…è´¸æ˜“
- è‚¡å¸‚åŠ¨æ€: Aè‚¡ã€æ¸¯è‚¡ã€ç¾è‚¡å¸‚åœºæ–°é—»
- è´¢æŠ¥è§£è¯»: ä¸Šå¸‚å…¬å¸ä¸šç»©ã€åˆ†æ
- è¡Œä¸šç ”ç©¶: æ¿å—è½®åŠ¨ã€è¡Œä¸šè¶‹åŠ¿
- å•†å“æœŸè´§: èƒ½æºã€é‡‘å±ã€å†œäº§å“
- å›½é™…å¸‚åœº: ç¾è”å‚¨ã€åœ°ç¼˜æ”¿æ²»ã€æ±‡ç‡
"""

import json
import re
import time
import random
from datetime import datetime, timezone, timedelta
from pathlib import Path
import hashlib
import subprocess
import html
import xml.etree.ElementTree as ET

PROJECT_ROOT = Path("/home/admin/.openclaw/workspace/tech-news")
DATA_DIR = PROJECT_ROOT / "data" / "finance" / "raw"
LOGS_DIR = PROJECT_ROOT / "logs" / "finance"

# å›½å†…è´¢ç»åª’ä½“
CN_SOURCES = [
    # ç»¼åˆè´¢ç»
    {"name": "æ–°æµªè´¢ç»", "url": "https://finance.sina.com.cn", "type": "finance_portal", "priority": "high", "category": ["å®è§‚", "è‚¡å¸‚", "æœŸè´§"]},
    {"name": "ä¸œæ–¹è´¢å¯Œ", "url": "https://www.eastmoney.com", "type": "finance_portal", "priority": "high", "category": ["è‚¡å¸‚", "åŸºé‡‘", "æœŸè´§"]},
    {"name": "åŒèŠ±é¡º", "url": "https://www.10jqka.com.cn", "type": "finance_portal", "priority": "high", "category": ["è‚¡å¸‚", "æŠ€æœ¯åˆ†æ"]},
    {"name": "è¯åˆ¸æ—¶æŠ¥", "url": "https://www.stcn.com", "type": "finance_media", "priority": "high", "category": ["è‚¡å¸‚", "å®è§‚", "æ”¿ç­–"]},
    {"name": "ä¸Šæµ·è¯åˆ¸æŠ¥", "url": "https://www.cnstock.com", "type": "finance_media", "priority": "high", "category": ["è‚¡å¸‚", "æ”¿ç­–", "IPO"]},
    {"name": "ä¸­å›½è¯åˆ¸æŠ¥", "url": "https://www.cs.com.cn", "type": "finance_media", "priority": "high", "category": ["è‚¡å¸‚", "å®è§‚"]},
    {"name": "ç¬¬ä¸€è´¢ç»", "url": "https://www.yicai.com", "type": "finance_media", "priority": "high", "category": ["å®è§‚", "è‚¡å¸‚", "å›½é™…"]},
    {"name": "ç»æµè§‚å¯ŸæŠ¥", "url": "https://www.eeo.com.cn", "type": "finance_media", "priority": "medium", "category": ["å®è§‚", "äº§ä¸š"]},
    {"name": "è´¢æ–°ç½‘", "url": "https://www.caixin.com", "type": "finance_media", "priority": "high", "category": ["å®è§‚", "é‡‘è", "äº§ä¸š"]},
    {"name": "21ä¸–çºªç»æµæŠ¥é“", "url": "https://www.21jingji.com", "type": "finance_media", "priority": "high", "category": ["å®è§‚", "äº§ä¸š", "è‚¡å¸‚"]},
    
    # è‚¡å¸‚ä¸“ä¸š
    {"name": "é›ªçƒ", "url": "https://xueqiu.com", "type": "stock_community", "priority": "medium", "category": ["è‚¡å¸‚", "æŠ•èµ„è§‚ç‚¹"]},
    {"name": "æ·˜è‚¡å§", "url": "https://www.taoguba.com.cn", "type": "stock_community", "priority": "medium", "category": ["è‚¡å¸‚", "çŸ­çº¿"]},
    
    # æœŸè´§ä¸“ä¸š
    {"name": "æœŸè´§æ—¥æŠ¥", "url": "https://www.qhrb.com.cn", "type": "futures_media", "priority": "high", "category": ["æœŸè´§", "å•†å“"]},
    {"name": "æ–‡åè´¢ç»", "url": "https://www.wenhua.com.cn", "type": "futures_media", "priority": "medium", "category": ["æœŸè´§", "æŠ€æœ¯åˆ†æ"]},
    
    # åŸºé‡‘/èµ„ç®¡
    {"name": "ä¸­å›½åŸºé‡‘æŠ¥", "url": "https://www.chnfund.com.cn", "type": "fund_media", "priority": "high", "category": ["åŸºé‡‘", "æœºæ„"]},
]

# å›½é™…è´¢ç»åª’ä½“
INTL_SOURCES = [
    # ç»¼åˆè´¢ç»
    {"name": "Bloomberg", "url": "https://www.bloomberg.com", "type": "international", "priority": "high", "category": ["å›½é™…", "å®è§‚", "è‚¡å¸‚"], "rss": "https://www.bloomberg.com/feed/podcast/bloomberg-markets.xml"},
    {"name": "Reuters", "url": "https://www.reuters.com", "type": "international", "priority": "high", "category": ["å›½é™…", "å®è§‚", "è‚¡å¸‚"], "rss": "https://www.reutersagency.com/feed/?taxonomy=best-topics&post_type=best"},
    {"name": "WSJ", "url": "https://www.wsj.com", "type": "international", "priority": "high", "category": ["å›½é™…", "è‚¡å¸‚", "å®è§‚"]},
    {"name": "FT", "url": "https://www.ft.com", "type": "international", "priority": "high", "category": ["å›½é™…", "å®è§‚", "é‡‘è"]},
    {"name": "CNBC", "url": "https://www.cnbc.com", "type": "international", "priority": "high", "category": ["å›½é™…", "è‚¡å¸‚", "å®è§‚"], "rss": "https://www.cnbc.com/id/10000664/device/rss/rss.html"},
    
    # å¸‚åœºæ•°æ®
    {"name": "MarketWatch", "url": "https://www.marketwatch.com", "type": "international", "priority": "medium", "category": ["ç¾è‚¡", "æ•°æ®"], "rss": "https://www.marketwatch.com/rss/topstories"},
    {"name": "Yahoo Finance", "url": "https://finance.yahoo.com", "type": "international", "priority": "medium", "category": ["ç¾è‚¡", "æ•°æ®"]},
    {"name": "Seeking Alpha", "url": "https://seekingalpha.com", "type": "international", "priority": "medium", "category": ["ç¾è‚¡", "åˆ†æ"], "rss": "https://seekingalpha.com/market_currents.xml"},
    
    # å¤®è¡Œ/æ”¿ç­–
    {"name": "Federal Reserve", "url": "https://www.federalreserve.gov", "type": "policy", "priority": "high", "category": ["ç¾è”å‚¨", "æ”¿ç­–"]},
    {"name": "ECB", "url": "https://www.ecb.europa.eu", "type": "policy", "priority": "medium", "category": ["æ¬§æ´²", "æ”¿ç­–"]},
    
    # å•†å“/èƒ½æº
    {"name": "OilPrice", "url": "https://oilprice.com", "type": "commodity", "priority": "high", "category": ["èƒ½æº", "åŸæ²¹"], "rss": "https://oilprice.com/rss/main"},
    {"name": "Investing.com", "url": "https://www.investing.com", "type": "international", "priority": "medium", "category": ["å•†å“", "å¤–æ±‡", "æ•°æ®"], "rss": "https://www.investing.com/rss/news.rss"},
    
    # åŠ å¯†è´§å¸
    {"name": "CoinDesk", "url": "https://www.coindesk.com", "type": "crypto", "priority": "medium", "category": ["åŠ å¯†è´§å¸", "åŒºå—é“¾"], "rss": "https://www.coindesk.com/arc/outboundfeeds/rss/"},
    {"name": "Cointelegraph", "url": "https://cointelegraph.com", "type": "crypto", "priority": "medium", "category": ["åŠ å¯†è´§å¸"], "rss": "https://cointelegraph.com/rss"},
]

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 Chrome/120.0.0.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
]

def log(msg, level="INFO"):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] [{level}] {msg}")
    try:
        with open(LOGS_DIR / f"crawler_{datetime.now().strftime('%Y%m%d')}.log", "a", encoding="utf-8") as f:
            f.write(f"[{timestamp}] [{level}] {msg}\n")
    except:
        pass

def generate_id(text):
    return hashlib.md5(text.encode()).hexdigest()[:12]

def clean_text(text):
    if not text:
        return ""
    text = re.sub(r'<[^>]+>', '', text)
    try:
        text = html.unescape(text)
    except:
        pass
    return re.sub(r'\s+', ' ', text).strip()

def parse_pub_date(date_str):
    """è§£æå„ç§æ ¼å¼çš„å‘å¸ƒæ—¶é—´"""
    if not date_str:
        return None
    
    formats = [
        "%a, %d %b %Y %H:%M:%S %z",
        "%a, %d %b %Y %H:%M:%S GMT",
        "%Y-%m-%dT%H:%M:%S%z",
        "%Y-%m-%dT%H:%M:%SZ",
        "%Y-%m-%d %H:%M:%S",
        "%Y-%m-%d",
    ]
    
    for fmt in formats:
        try:
            dt = datetime.strptime(date_str.strip(), fmt)
            if dt.tzinfo:
                dt = dt.astimezone(timezone(timedelta(hours=8)))
            return dt.strftime("%Y-%m-%d %H:%M:%S")
        except:
            continue
    
    match = re.search(r'(\d{4}-\d{2}-\d{2})', date_str)
    if match:
        return match.group(1)
    
    return None

def extract_market_signal(title, content=""):
    """æå–å¸‚åœºä¿¡å· - æŠ•èµ„è€…è§†è§’"""
    text = (title + " " + content).lower()
    
    signals = {
        "bullish": [],  # åˆ©å¥½ä¿¡å·
        "bearish": [],  # åˆ©ç©ºä¿¡å·
        "neutral": []   # ä¸­æ€§/è§‚æœ›
    }
    
    # åˆ©å¥½å…³é”®è¯
    bullish_keywords = [
        "ä¸Šæ¶¨", "å¤§æ¶¨", "æš´æ¶¨", "æ–°é«˜", "çªç ´", "åˆ©å¥½", "ç›ˆåˆ©", "å¢é•¿", "è¶…é¢„æœŸ",
        "é™æ¯", "å®½æ¾", "åˆºæ¿€", "åå¼¹", "å›æš–", "æ¢å¤", "å¹¶è´­", "æ”¶è´­",
        "å¢æŒ", "å›è´­", "åˆ†çº¢", "ä¸šç»©å¤§å¢", "æ‰­äº", "è®¢å•", "ä¸­æ ‡",
        "surge", "rally", "gain", "profit", "growth", "beat", "rise"
    ]
    
    # åˆ©ç©ºå…³é”®è¯
    bearish_keywords = [
        "ä¸‹è·Œ", "å¤§è·Œ", "æš´è·Œ", "æ–°ä½", "ç ´ä½", "åˆ©ç©º", "äºæŸ", "ä¸‹æ»‘", "ä¸åŠé¢„æœŸ",
        "åŠ æ¯", "æ”¶ç´§", "èç¼©", "è¡°é€€", "è£å‘˜", "ç ´äº§", "è¿çº¦", "æš´é›·",
        "å‡æŒ", "æŠ›å”®", "é€€å¸‚", "è°ƒæŸ¥", "å¤„ç½š", "è¯‰è®¼", "ç½šæ¬¾",
        "plunge", "crash", "drop", "loss", "down", "recession", "fear"
    ]
    
    for kw in bullish_keywords:
        if kw in text:
            signals["bullish"].append(kw)
    
    for kw in bearish_keywords:
        if kw in text:
            signals["bearish"].append(kw)
    
    # ç¡®å®šæ•´ä½“ä¿¡å·
    if len(signals["bullish"]) > len(signals["bearish"]):
        signals["overall"] = "bullish"
    elif len(signals["bearish"]) > len(signals["bullish"]):
        signals["overall"] = "bearish"
    else:
        signals["overall"] = "neutral"
    
    return signals

def extract_entities(title, content=""):
    """æå–å…³é”®å®ä½“ - è‚¡ç¥¨ã€å…¬å¸ã€è¡Œä¸š"""
    text = title + " " + content
    
    entities = {
        "stocks": [],
        "companies": [],
        "sectors": [],
        "indices": []
    }
    
    # ä¸»è¦æŒ‡æ•°
    indices_patterns = [
        (r"ä¸Šè¯æŒ‡æ•°|æ²ªæŒ‡|ä¸Šè¯", "ä¸Šè¯æŒ‡æ•°"),
        (r"æ·±è¯æˆæŒ‡|æ·±æˆæŒ‡", "æ·±è¯æˆæŒ‡"),
        (r"åˆ›ä¸šæ¿æŒ‡|åˆ›ä¸šæ¿", "åˆ›ä¸šæ¿æŒ‡"),
        (r"ç§‘åˆ›50|ç§‘åˆ›æ¿", "ç§‘åˆ›50"),
        (r"æ’ç”ŸæŒ‡æ•°|æ’æŒ‡", "æ’ç”ŸæŒ‡æ•°"),
        (r"é“ç¼æ–¯|é“æŒ‡", "é“ç¼æ–¯"),
        (r"çº³æ–¯è¾¾å…‹|çº³æŒ‡", "çº³æ–¯è¾¾å…‹"),
        (r"æ ‡æ™®500|S&P", "æ ‡æ™®500"),
    ]
    
    for pattern, name in indices_patterns:
        if re.search(pattern, text):
            entities["indices"].append(name)
    
    # è¡Œä¸šæ¿å—
    sectors = [
        "åŠå¯¼ä½“", "èŠ¯ç‰‡", "æ–°èƒ½æº", "å…‰ä¼", "é”‚ç”µæ± ", "å‚¨èƒ½", "é£ç”µ",
        "ç™½é…’", "åŒ»è¯", "ç”Ÿç‰©åˆ¶è¯", "åŒ»ç–—å™¨æ¢°", "ä¸­è¯",
        "é“¶è¡Œ", "åˆ¸å•†", "ä¿é™©", "åœ°äº§", "æˆ¿åœ°äº§",
        "æ±½è½¦", "æ–°èƒ½æºæ±½è½¦", "æ™ºèƒ½é©¾é©¶",
        "æ¶ˆè´¹ç”µå­", "è‹¹æœäº§ä¸šé“¾", "æ¶ˆè´¹", "é£Ÿå“é¥®æ–™",
        "å†›å·¥", "èˆªå¤©", "é€šä¿¡", "5G", "äººå·¥æ™ºèƒ½", "AI",
        "äº’è”ç½‘", "ç”µå•†", "æ¸¸æˆ", "ä¼ åª’", "æ•™è‚²",
        "æœ‰è‰²", "ç…¤ç‚­", "çŸ³æ²¹", "åŒ–å·¥", "é’¢é“"
    ]
    
    for sector in sectors:
        if sector in text:
            entities["sectors"].append(sector)
    
    # å¤§å…¬å¸
    companies = [
        "èŒ…å°", "å®å¾·æ—¶ä»£", "æ¯”äºšè¿ª", "è…¾è®¯", "é˜¿é‡Œ", "å­—èŠ‚", "ç¾å›¢",
        "åä¸º", "å°ç±³", "è”šæ¥", "ç†æƒ³", "å°é¹", "ä¸­èŠ¯å›½é™…",
        "è‹¹æœ", "ç‰¹æ–¯æ‹‰", "è‹±ä¼Ÿè¾¾", "å¾®è½¯", "è°·æ­Œ", "Meta", "äºšé©¬é€Š"
    ]
    
    for company in companies:
        if company in text:
            entities["companies"].append(company)
    
    return entities

def fetch_rss(source):
    """é€šè¿‡ RSS è·å–æ•°æ®"""
    rss_url = source.get("rss")
    if not rss_url:
        return []
    
    log(f"RSS è·å–: {source['name']}")
    
    try:
        cmd = ["curl", "-s", "-L",
               "--connect-timeout", "15",
               "--max-time", "30",
               "-H", f"User-Agent: {random.choice(USER_AGENTS)}",
               rss_url]
        
        result = subprocess.run(cmd, capture_output=True, timeout=45)
        
        if not result.stdout or len(result.stdout) < 100:
            return []
        
        content = result.stdout.decode('utf-8', errors='replace')
        
        articles = []
        root = ET.fromstring(content)
        
        for item in root.findall('.//item')[:50]:
            try:
                title_elem = item.find('title')
                link_elem = item.find('link')
                pub_date_elem = item.find('pubDate')
                desc_elem = item.find('description')
                
                if title_elem is None or link_elem is None:
                    continue
                
                title = clean_text(title_elem.text) if title_elem.text else ""
                url = link_elem.text.strip() if link_elem.text else ""
                pub_date = parse_pub_date(pub_date_elem.text) if pub_date_elem is not None else None
                
                if not title or len(title) < 5 or not url:
                    continue
                
                # æå–å¸‚åœºä¿¡å·
                signals = extract_market_signal(title)
                entities = extract_entities(title)
                
                article = {
                    "id": generate_id(url + title),
                    "title": title,
                    "url": url,
                    "source": source["name"],
                    "categories": source.get("category", []),
                    "pub_date": pub_date,
                    "crawl_time": datetime.now().isoformat(),
                    "date": datetime.now().strftime("%Y-%m-%d"),
                    "market_signal": signals,
                    "entities": entities
                }
                
                articles.append(article)
                
            except Exception as e:
                continue
        
        log(f"{source['name']} (RSS): {len(articles)} æ¡")
        return articles
        
    except subprocess.TimeoutExpired:
        log(f"RSS è¶…æ—¶: {source['name']}", "WARN")
        return []
    except Exception as e:
        log(f"RSS é”™è¯¯ {source['name']}: {str(e)[:50]}", "WARN")
        return []

def fetch_html(source):
    """é€šè¿‡ HTML é¡µé¢è·å–æ•°æ®"""
    url = source["url"]
    name = source["name"]
    
    log(f"HTML è·å–: {name}")
    
    try:
        cmd = ["curl", "-s", "-L",
               "--connect-timeout", "10",
               "--max-time", "25",
               "-H", f"User-Agent: {random.choice(USER_AGENTS)}",
               "-H", "Accept: text/html,*/*",
               url]
        
        result = subprocess.run(cmd, capture_output=True, timeout=35)
        
        if not result.stdout or len(result.stdout) < 500:
            return []
        
        for enc in ['utf-8', 'gbk', 'gb2312', 'latin-1']:
            try:
                content = result.stdout.decode(enc)
                break
            except:
                continue
        else:
            content = result.stdout.decode('utf-8', errors='replace')
        
        pattern = r'<a[^>]*href=["\']([^"\']+)["\'][^>]*>([^<]+)</a>'
        links = re.findall(pattern, content, re.IGNORECASE)
        
        base_domain = re.match(r'https?://([^/]+)', url)
        base_domain = base_domain.group(1) if base_domain else ""
        
        articles = []
        seen_urls = set()
        
        for href, title in links[:150]:
            href = href.strip()
            if href.startswith("/"):
                href = f"https://{base_domain}{href}"
            elif not href.startswith("http"):
                continue
            
            if href in seen_urls:
                continue
            seen_urls.add(href)
            
            title = clean_text(title)
            
            signals = extract_market_signal(title)
            entities = extract_entities(title)
            
            article = {
                "id": generate_id(href + title),
                "title": title,
                "url": href,
                "source": name,
                "categories": source.get("category", []),
                "pub_date": None,
                "crawl_time": datetime.now().isoformat(),
                "date": datetime.now().strftime("%Y-%m-%d"),
                "market_signal": signals,
                "entities": entities
            }
            
            if validate_article(article):
                articles.append(article)
            
            if len(articles) >= 80:
                break
        
        log(f"{name} (HTML): {len(articles)} æ¡")
        return articles
        
    except subprocess.TimeoutExpired:
        log(f"HTML è¶…æ—¶: {name}", "WARN")
        return []
    except Exception as e:
        log(f"HTML é”™è¯¯ {name}: {str(e)[:50]}", "ERROR")
        return []

def validate_article(article):
    """éªŒè¯æ–‡ç« æœ‰æ•ˆæ€§"""
    title = article.get("title", "")
    url = article.get("url", "")
    
    if not title or len(title) < 5 or len(title) > 200:
        return False
    if not url or len(url) < 10:
        return False
    
    skip_words = ["ç™»å½•", "æ³¨å†Œ", "é¦–é¡µ", "æ›´å¤š", "åˆ†äº«", "æ”¶è—", "å¾®ä¿¡", "å¾®åš", "APP",
                  "ä¸‹ä¸€é¡µ", "ä¸Šä¸€é¡µ", "Subscribe", "Login", "Sign Up", "RSS", "About",
                  "å¹¿å‘Š", "åˆä½œ", "è”ç³»æˆ‘ä»¬", "ç‰ˆæƒ"]
    if any(w in title for w in skip_words):
        return False
    
    # å¿…é¡»åŒ…å«è´¢ç»ç›¸å…³å…³é”®è¯
    finance_keywords = [
        "è‚¡", "å¸‚", "é‡‘", "è´¢", "ç»", "æŠ•èµ„", "åŸºé‡‘", "æœŸè´§", "å€º", "æ±‡ç‡",
        "åˆ©ç‡", "é€šèƒ€", "GDP", "å¤®è¡Œ", "é“¶è¡Œ", "ä¸Šå¸‚", "è´¢æŠ¥", "ä¸šç»©",
        "èèµ„", "å¹¶è´­", "IPO", "è¯åˆ¸", "æŒ‡æ•°", "æ¿å—", "æ¶¨", "è·Œ",
        "stock", "market", "invest", "fund", "trade", "economy", "finance",
        "rate", "bond", "currency", "profit", "loss", "earnings"
    ]
    
    if not any(kw in title.lower() for kw in finance_keywords):
        return False
    
    return True

def crawl_source(source):
    """çˆ¬å–å•ä¸ªæ•°æ®æº"""
    if source.get("rss"):
        articles = fetch_rss(source)
        if articles:
            return articles
    return fetch_html(source)

def main():
    log("=" * 60)
    log("è´¢ç»èµ„è®¯çˆ¬è™« v1 å¯åŠ¨ (æŠ•èµ„è€…è§†è§’)")
    log("=" * 60)
    
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    LOGS_DIR.mkdir(parents=True, exist_ok=True)
    
    all_articles = []
    
    log("\nğŸ“ çˆ¬å–å›½å†…è´¢ç»æ•°æ®æº...")
    for source in CN_SOURCES:
        try:
            articles = crawl_source(source)
            all_articles.extend(articles)
            time.sleep(random.uniform(0.3, 0.8))
        except Exception as e:
            log(f"çˆ¬å– {source['name']} å¼‚å¸¸: {e}", "ERROR")
    
    log("\nğŸŒ çˆ¬å–å›½é™…è´¢ç»æ•°æ®æº...")
    for source in INTL_SOURCES:
        try:
            articles = crawl_source(source)
            all_articles.extend(articles)
            time.sleep(random.uniform(0.5, 1.0))
        except Exception as e:
            log(f"çˆ¬å– {source['name']} å¼‚å¸¸: {e}", "ERROR")
    
    # å»é‡
    seen = set()
    unique = []
    for a in all_articles:
        key = a["id"] + a["title"][:20]
        if key not in seen:
            seen.add(key)
            unique.append(a)
    
    # ç»Ÿè®¡
    source_stats = {}
    signal_stats = {"bullish": 0, "bearish": 0, "neutral": 0}
    
    for a in unique:
        src = a.get("source", "æœªçŸ¥")
        source_stats[src] = source_stats.get(src, 0) + 1
        
        signal = a.get("market_signal", {}).get("overall", "neutral")
        signal_stats[signal] = signal_stats.get(signal, 0) + 1
    
    # ä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = DATA_DIR / f"finance_{timestamp}.json"
    
    output_data = {
        "crawl_time": datetime.now().isoformat(),
        "version": "v1",
        "total_articles": len(unique),
        "cn_sources": [s["name"] for s in CN_SOURCES],
        "intl_sources": [s["name"] for s in INTL_SOURCES],
        "source_stats": source_stats,
        "signal_stats": signal_stats,
        "articles": unique
    }
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    log("\n" + "=" * 60)
    log(f"âœ… å®Œæˆ! å…± {len(unique)} æ¡èµ„è®¯")
    log(f"ğŸ“ ä¿å­˜: {output_file}")
    log("\nğŸ“Š å¸‚åœºæƒ…ç»ªä¿¡å·:")
    log(f"   åˆ©å¥½: {signal_stats['bullish']} æ¡")
    log(f"   åˆ©ç©º: {signal_stats['bearish']} æ¡")
    log(f"   ä¸­æ€§: {signal_stats['neutral']} æ¡")
    log("\nğŸ“Š æ•°æ®æºç»Ÿè®¡:")
    for src, count in sorted(source_stats.items(), key=lambda x: x[1], reverse=True)[:10]:
        log(f"   {src}: {count} æ¡")
    
    return unique

if __name__ == "__main__":
    main()